---
title: "p8105_hw5_yf2563"
author: "Yatong Feng"
date: "11/18/2020"
output: github_document
---

## Problem 0 - Setup
```{r setup, message = F}
library(tidyverse)
library(stringr)

knitr::opts_chunk$set(
  message = F,
  warning = F,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

#### 1.1.1 

Describe the raw data. 

```{r}
homicide_raw_df = 
  read_csv("data-homicides/homicide-data.csv")
```

**Brief Introduction:**

- The data included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim.
- The dimension of this resulting dataset is (`r dim(homicide_raw_df)`), which means that, there are `r dim(homicide_raw_df)[1]` observations and `r dim(homicide_raw_df)[2]` variables.
- The variables include `r ls(homicide_raw_df)`. 

**Create a city_state variable:**
```{r}
homicide_df = 
  homicide_raw_df %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```


#### 1.1.2 

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides 

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )

aggregate_df
```


#### 1.2

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```


#### 1.3 

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)

results_df
```


#### 1.4

Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Problem 2

longitudinal study: include a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

#### 2.1 

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

- dataframe: containing all file names (list.files function)
- Iterate over file names, read in data for each subject (purrr::map) and saving the result as a new variable in the dataframe

```{r}
path_df = 
  tibble(
    path = list.files("lda_data"),
  ) %>% 
  mutate(
    path = str_c("lda_data/", path),
    map_df(.x = path, read_csv)) %>% 
  bind_cols()

path_df
```

- Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary


```{r}
prob2_tidy_df = 
  path_df %>% 
  mutate(path = str_sub(path, 10, -5)) %>% 
  separate(path,
           into = c("control_arm", "subject_id"),
           sep = "_") %>% 
  mutate(control_arm = case_when(
    control_arm == "con" ~ "Control",
    control_arm == "exp" ~ "Expose"
  )) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "value")

prob2_tidy_df
```

- Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.