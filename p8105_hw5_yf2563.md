p8105\_hw5\_yf2563
================
Yatong Feng
11/18/2020

## Problem 0 - Setup

``` r
library(tidyverse)
library(stringr)

knitr::opts_chunk$set(
  message = F,
  warning = F,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

#### 1.1.1

Describe the raw data.

``` r
homicide_raw_df = 
  read_csv("data-homicides/homicide-data.csv")
```

**Brief Introduction:**

  - The data included the location of the killing, whether an arrest was
    made and, in most cases, basic demographic information about each
    victim.
  - The dimension of this resulting dataset is (52179, 12), which means
    that, there are 52179 observations and 12 variables.
  - The variables include city, disposition, lat, lon, reported\_date,
    state, uid, victim\_age, victim\_first, victim\_last, victim\_race,
    victim\_sex.

**Create a city\_state variable:**

``` r
homicide_df = 
  homicide_raw_df %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

#### 1.1.2

Summarize within cities to obtain the total number of homicides and the
number of unsolved homicides

``` r
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )

aggregate_df
```

    ## # A tibble: 50 x 3
    ##    city_state     hom_total hom_unsolved
    ##    <chr>              <int>        <int>
    ##  1 Albuquerque_NM       378          146
    ##  2 Atlanta_GA           973          373
    ##  3 Baltimore_MD        2827         1825
    ##  4 Baton Rouge_LA       424          196
    ##  5 Birmingham_AL        800          347
    ##  6 Boston_MA            614          310
    ##  7 Buffalo_NY           521          319
    ##  8 Charlotte_NC         687          206
    ##  9 Chicago_IL          5535         4073
    ## 10 Cincinnati_OH        694          309
    ## # … with 40 more rows

#### 1.2

For the city of Baltimore, MD, use the prop.test function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the broom::tidy to this object and pull
the estimated proportion and confidence intervals from the resulting
tidy dataframe.

``` r
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

    ## # A tibble: 1 x 8
    ##   estimate statistic  p.value parameter conf.low conf.high method    alternative
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample… two.sided

#### 1.3

Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a “tidy” pipeline, making use of purrr::map,
purrr::map2, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

``` r
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)

results_df
```

    ## # A tibble: 50 x 4
    ##    city_state     estimate conf.low conf.high
    ##    <chr>             <dbl>    <dbl>     <dbl>
    ##  1 Albuquerque_NM    0.386    0.337     0.438
    ##  2 Atlanta_GA        0.383    0.353     0.415
    ##  3 Baltimore_MD      0.646    0.628     0.663
    ##  4 Baton Rouge_LA    0.462    0.414     0.511
    ##  5 Birmingham_AL     0.434    0.399     0.469
    ##  6 Boston_MA         0.505    0.465     0.545
    ##  7 Buffalo_NY        0.612    0.569     0.654
    ##  8 Charlotte_NC      0.300    0.266     0.336
    ##  9 Chicago_IL        0.736    0.724     0.747
    ## 10 Cincinnati_OH     0.445    0.408     0.483
    ## # … with 40 more rows

#### 1.4

Create a plot that shows the estimates and CIs for each city – check out
geom\_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

``` r
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw5_yf2563_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

## Problem 2

longitudinal study: include a control arm and an experimental arm. Data
for each participant is included in a separate file, and file names
include the subject ID and arm.

#### 2.1

Create a tidy dataframe containing data from all participants, including
the subject ID, arm, and observations over time:

  - dataframe: containing all file names (list.files function)
  - Iterate over file names, read in data for each subject (purrr::map)
    and saving the result as a new variable in the dataframe

<!-- end list -->

``` r
path_df = 
  tibble(
    path = list.files("lda_data"),
  ) %>% 
  mutate(
    path = str_c("lda_data/", path),
    map_df(.x = path, read_csv)) %>% 
  bind_cols()

path_df
```

    ## # A tibble: 20 x 9
    ##    path                week_1 week_2 week_3 week_4 week_5 week_6 week_7 week_8
    ##    <chr>                <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
    ##  1 lda_data/con_01.csv   0.2  -1.31    0.66   1.96   0.23   1.09   0.05   1.94
    ##  2 lda_data/con_02.csv   1.13 -0.88    1.07   0.17  -0.83  -0.31   1.58   0.44
    ##  3 lda_data/con_03.csv   1.77  3.11    2.22   3.26   3.31   0.89   1.88   1.01
    ##  4 lda_data/con_04.csv   1.04  3.66    1.22   2.33   1.47   2.7    1.87   1.66
    ##  5 lda_data/con_05.csv   0.47 -0.580  -0.09  -1.37  -0.32  -2.17   0.45   0.48
    ##  6 lda_data/con_06.csv   2.37  2.5     1.59  -0.16   2.08   3.07   0.78   2.35
    ##  7 lda_data/con_07.csv   0.03  1.21    1.13   0.64   0.49  -0.12  -0.07   0.46
    ##  8 lda_data/con_08.csv  -0.08  1.42    0.09   0.36   1.18  -1.16   0.33  -0.44
    ##  9 lda_data/con_09.csv   0.08  1.24    1.44   0.41   0.95   2.75   0.3    0.03
    ## 10 lda_data/con_10.csv   2.14  1.15    2.52   3.44   4.26   0.97   2.73  -0.53
    ## 11 lda_data/exp_01.csv   3.05  3.67    4.84   5.8    6.33   5.46   6.38   5.91
    ## 12 lda_data/exp_02.csv  -0.84  2.63    1.64   2.58   1.24   2.32   3.11   3.78
    ## 13 lda_data/exp_03.csv   2.15  2.08    1.82   2.84   3.36   3.61   3.37   3.74
    ## 14 lda_data/exp_04.csv  -0.62  2.54    3.78   2.73   4.49   5.82   6      6.49
    ## 15 lda_data/exp_05.csv   0.7   3.33    5.34   5.57   6.9    6.66   6.24   6.95
    ## 16 lda_data/exp_06.csv   3.73  4.08    5.4    6.41   4.87   6.09   7.66   5.83
    ## 17 lda_data/exp_07.csv   1.18  2.35    1.23   1.17   2.02   1.61   3.13   4.88
    ## 18 lda_data/exp_08.csv   1.37  1.43    1.84   3.6    3.8    4.72   4.68   5.7 
    ## 19 lda_data/exp_09.csv  -0.4   1.08    2.66   2.7    2.8    2.64   3.51   3.27
    ## 20 lda_data/exp_10.csv   1.09  2.8     2.8    4.3    2.25   6.57   6.09   4.64

  - Tidy the result; manipulate file names to include control arm and
    subject ID, make sure weekly observations are “tidy”, and do any
    other tidying that’s necessary

<!-- end list -->

``` r
prob2_tidy_df = 
  path_df %>% 
  mutate(path = str_sub(path, 10, -5)) %>% 
  separate(path,
           into = c("control_arm", "subject_id"),
           sep = "_") %>% 
  mutate(control_arm = case_when(
    control_arm == "con" ~ "Control",
    control_arm == "exp" ~ "Experiment"
  )) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "value")

prob2_tidy_df
```

    ## # A tibble: 160 x 4
    ##    control_arm subject_id week   value
    ##    <chr>       <chr>      <chr>  <dbl>
    ##  1 Control     01         week_1  0.2 
    ##  2 Control     01         week_2 -1.31
    ##  3 Control     01         week_3  0.66
    ##  4 Control     01         week_4  1.96
    ##  5 Control     01         week_5  0.23
    ##  6 Control     01         week_6  1.09
    ##  7 Control     01         week_7  0.05
    ##  8 Control     01         week_8  1.94
    ##  9 Control     02         week_1  1.13
    ## 10 Control     02         week_2 -0.88
    ## # … with 150 more rows

  - Make a spaghetti plot showing observations on each subject over
    time, and comment on differences between groups.

<!-- end list -->

``` r
prob2_tidy_df %>% 
  mutate(week = str_replace(week, "week_", "")) %>%
  ggplot(aes(x = week, y = value, group = subject_id, color = subject_id)) +
  geom_point() +
  geom_line() +
  facet_grid(~control_arm)
```

<img src="p8105_hw5_yf2563_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

The control and experiment group have different trend. - For the control
group: relatively stable around 1.25. - For the expose group: increasing
trend over time.

## Problem 3

When designing an experiment or analysis, a common question is whether
it is likely that a true effect will be detected – put differently,
whether a false null hypothesis will be rejected. The probability that a
false null hypothesis is rejected is referred to as power, and it
depends on several factors, including: the sample size; the effect size;
and the error variance. In this problem, you will conduct a simulation
to explore power in a one-sample t-test.

First set the following design elements:

Fix n=30, sigma=5, mu=0. Generate 5000 datasets from the model

``` r
set.seed(520)

sim_t_test = function(samp_size = 30, mu = 0, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(samp_size, mean = mu, sd = sigma),
  )
  
  sim_data  = t.test(sim_data) %>% 
  broom::tidy() %>% 
  mutate(mu = mu) %>% 
  select(mu, estimate, p.value)
  
  sim_data
  
  
}

# when mu = 0, generate 5000 results
sim_0 = map_dfr(0, ~rerun(5000, sim_t_test(mu = .x)))
sim_0
```

    ## # A tibble: 5,000 x 3
    ##       mu estimate p.value
    ##    <dbl>    <dbl>   <dbl>
    ##  1     0  -1.05     0.221
    ##  2     0   0.572    0.517
    ##  3     0   0.964    0.392
    ##  4     0  -0.505    0.566
    ##  5     0  -0.257    0.759
    ##  6     0  -0.674    0.430
    ##  7     0  -1.04     0.225
    ##  8     0   1.15     0.233
    ##  9     0   0.0167   0.984
    ## 10     0   0.307    0.756
    ## # … with 4,990 more rows

``` r
# result for mu = {0,1,2,3,4,5,6}

output = map_dfr(0:6, ~rerun(5000, sim_t_test(mu = .x)))
output
```

    ## # A tibble: 35,000 x 3
    ##       mu estimate p.value
    ##    <int>    <dbl>   <dbl>
    ##  1     0   0.265   0.764 
    ##  2     0  -0.194   0.855 
    ##  3     0  -0.142   0.845 
    ##  4     0   0.726   0.475 
    ##  5     0   0.484   0.524 
    ##  6     0  -0.0390  0.960 
    ##  7     0   0.948   0.372 
    ##  8     0  -0.542   0.436 
    ##  9     0  -0.991   0.246 
    ## 10     0   1.80    0.0608
    ## # … with 34,990 more rows

#### Plot 1

  - Make a plot showing the proportion of times the null was rejected
    (the power of the test) on the y axis and the true value of mu on
    the x axis. Describe the association between effect size and power.

<!-- end list -->

``` r
plot1_df = 
output %>% 
    group_by(mu) %>% 
    filter(p.value < 0.05) %>% 
    summarize(proportion = n()/5000)
  
plot1_df %>% 
  ggplot(aes(x = mu, y = proportion))+
  geom_point()+
  geom_line() + 
  labs(
    title = "Association between effect size and power",
    x = "True mean",
    y = "The power of the test"
  )
```

<img src="p8105_hw5_yf2563_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />

**Comments:**

The plot have an obvious increasing trend. With the increase of the true
mean, he proportion of times the null was rejected (the power of the
test) also increase. The association between effect size and power is
positive

#### Plot2

  - Make a plot showing the average estimate of mu on the y axis and the
    true value of mu on the x axis.
  - Make a second plot (or overlay on the first) the average estimate of
    mu only in samples for which the null was rejected on the y axis and
    the true value of mu on the x axis.

<!-- end list -->

``` r
average_estimate_rejected_df =
  output %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarise(average_estimate_rejected = mean(estimate))

average_estimate_df =
  output %>% 
  group_by(mu) %>% 
  summarise(average_estimate = mean(estimate)) %>% 
  mutate(average_estimate_rejected = pull(average_estimate_rejected_df, average_estimate_rejected)) %>% 
  pivot_longer(
    average_estimate:average_estimate_rejected,
    names_to = "type",
    values_to = "Average_estimate"
  )
```

``` r
average_estimate_df %>% 
ggplot(aes(x = mu, y = Average_estimate, color = type)) +
  geom_point() +
  geom_line() + 
  labs(
    title = "Simulation difference among two groups",
    x = "True mean",
    y = "Average estimate of mu"
  )
```

<img src="p8105_hw5_yf2563_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />

  - Is the sample average of mu across tests for which the null is
    rejected approximately equal to the true value of mu? Why or why
    not?

**Comments:** - for mu = 0, the sample average of mu across tests for
which the null is rejected is relatively same as the true value of mu,
since the null hypothesis is mu = 0

  - for mu \> 0, the sample average of mu across tests for which the
    null is rejected is approximately equal to the true value of mu with
    the increase of the true mean. Since from from CLT and as the mu
    increase, the distribution of sample mean will follows a normal
    distribution with increasing mu and the rejection proportion of will
    get bigger.
